{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 1: time-advancing scheme and neural networks\n",
    "\n",
    "Exercise on the implementation of Neural ODEs. \n",
    "\n",
    "Author: Stefano Pagani <stefano.pagani@polimi.it>.\n",
    "\n",
    "Date: 2024\n",
    "\n",
    "Course: Mathematical and numerical foundations of scientific machine learning.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Let us consider the Lorenz system of differential equations:\n",
    "\n",
    "$\n",
    "\\dot{x} = \\sigma (y-x) \\\\\n",
    "\\dot{y} = x (\\rho-z) - y \\\\\n",
    "\\dot{z} = x y - \\beta z\n",
    "$\n",
    "\n",
    "with parameters $\\sigma = 10$, $\\rho = 28$, and $\\beta = 8/3$.\n",
    "\n",
    "The goal of this exercise is to implement a feed-forward neural network that learns the right-hand side of a dynamical system, which allows to approximate the dynamics of the state.\n",
    "\n",
    "Complete the notebook accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# imports\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "from scipy import integrate\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import backend as K\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rcParams.update({'font.size': 18})\n",
    "plt.rcParams['figure.figsize'] = [9, 9]\n",
    "\n",
    "## Simulate the Lorenz System for various initial conditions\n",
    "\n",
    "dt = 0.01\n",
    "T = 1.0\n",
    "t = np.arange(0,T+dt,dt)\n",
    "beta = 8/3\n",
    "sigma = 10\n",
    "rho = 28\n",
    "\n",
    "n_samples = 10\n",
    "n_valid = 5\n",
    "\n",
    "nn_input = np.zeros((n_samples*(len(t)-1),3))\n",
    "nn_output = np.zeros_like(nn_input)\n",
    "\n",
    "nn_input_val = np.zeros((n_valid*(len(t)-1),3))\n",
    "nn_output_val = np.zeros_like(nn_input_val)\n",
    "\n",
    "nn_pred = np.zeros_like(nn_input_val)\n",
    "\n",
    "def lorenz_deriv(t0, x_y_z, sigma=sigma, beta=beta, rho=rho):\n",
    "    x, y, z = x_y_z\n",
    "    return [sigma * (y - x), x * (rho - z) - y, x * y - beta * z]\n",
    "\n",
    "np.random.seed(123)\n",
    "x0 = 1.5 + 3.0 * np.random.random((n_samples, 3))\n",
    "x0_val = 1.5 + 3.0 * np.random.random((n_valid, 3))\n",
    "\n",
    "x_t = np.asarray([integrate.solve_ivp(lorenz_deriv, t_span=(0,T), y0=x0_j, t_eval=t).y for x0_j in x0])\n",
    "\n",
    "x_t_val = np.asarray([integrate.solve_ivp(lorenz_deriv, (0,T), y0=x0_j, t_eval=t).y for x0_j in x0_val])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,1,subplot_kw={'projection': '3d'})\n",
    "\n",
    "# training data\n",
    "for j in range(n_samples):\n",
    "\n",
    "    nn_input[j*(len(t)-1):(j+1)*(len(t)-1),:] = x_t[j,:,:-1].T\n",
    "    nn_output[j*(len(t)-1):(j+1)*(len(t)-1),:] = x_t[j,:,1:].T\n",
    "\n",
    "    x, y, z = x_t[j,:,:]\n",
    "    ax.plot(x, y, z,linewidth=1)\n",
    "    ax.scatter(x0[j,0],x0[j,1],x0[j,2],color='r')\n",
    "\n",
    "ax.view_init(18, -113)\n",
    "plt.show()\n",
    "\n",
    "# test data\n",
    "for j in range(n_valid):\n",
    "    nn_input_val[j*(len(t)-1):(j+1)*(len(t)-1),:] = x_t_val[j,:,:-1].T\n",
    "    nn_output_val[j*(len(t)-1):(j+1)*(len(t)-1),:] = x_t_val[j,:,1:].T\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to tensor\n",
    "\n",
    "X_train = tf.convert_to_tensor(x_t[:,:,:-1],dtype=tf.float64)\n",
    "Y_train = tf.convert_to_tensor(x_t[:,:,1:],dtype=tf.float64)\n",
    "\n",
    "X_valid = tf.convert_to_tensor(x_t_val[:,:,:-1],dtype=tf.float64)\n",
    "Y_valid = tf.convert_to_tensor(x_t_val[:,:,1:],dtype=tf.float64)\n",
    "\n",
    "Y_pred = tf.convert_to_tensor(nn_pred,dtype=tf.float64)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  loss function\n",
    "def loss(X_train,Y_train):\n",
    "\n",
    "    # single time step loss\n",
    "    for k in range(100):\n",
    "        if k==0:\n",
    "            Y_pred = tf.expand_dims(Updatemodel(X_train[:,:,0]),2)\n",
    "        else:\n",
    "            Y_pred = tf.concat( [Y_pred, tf.expand_dims(Updatemodel(X_train[:,:,k]),2) ], 2 )\n",
    "\n",
    "    # loss components\n",
    "    mse_loss = tf.reduce_mean(tf.pow(Y_pred-Y_train,2))\n",
    "\n",
    "    return mse_loss\n",
    "\n",
    "# neural network weight gradients\n",
    "@tf.function\n",
    "def grad(model,X_train,Y_train):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        loss_value = loss(X_train,Y_train)\n",
    "        grads = tape.gradient(loss_value,model.trainable_variables)\n",
    "    return loss_value, grads\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: Compare the following models:\n",
    "\n",
    "  1. $\\bf{x}^{(k+1)} = \\mathcal{NN}(\\bf{x}^{(k)}; \\bf{W})  $\n",
    "  2. $\\bf{x}^{(k+1)} = \\bf{x}^{(k)} + \\Delta t \\mathcal{NN}(\\bf{x}^{(k)}; \\bf{W}) $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dense = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(8, activation=None, input_shape=(3,),kernel_initializer=\"glorot_normal\",dtype=tf.float64),\n",
    "    tf.keras.layers.Dense(16, activation='tanh',kernel_initializer=\"glorot_normal\",dtype=tf.float64),\n",
    "    tf.keras.layers.Dense(3,activation=None,kernel_initializer=\"glorot_normal\",dtype=tf.float64)\n",
    "])\n",
    "\n",
    "# inputs      = tf.keras.Input(shape=(3,))\n",
    "# outputs     = dense(inputs)\n",
    "# Updatemodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "inputs      = tf.keras.Input(shape=(3,),dtype=tf.float64)\n",
    "outputs     = inputs[:,0:3] + 0.01*dense(inputs)\n",
    "Updatemodel = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "tf_optimizer = tf.keras.optimizers.Adam(learning_rate=0.003,beta_1=0.99)\n",
    "\n",
    "for iter in range(10000):\n",
    "\n",
    "  # compute gradients using AD\n",
    "  loss_value,grads = grad(Updatemodel,X_train,Y_train)\n",
    "\n",
    "  # update neural network weights\n",
    "  tf_optimizer.apply_gradients(zip(grads,Updatemodel.trainable_variables))\n",
    "\n",
    "  # display intermediate results\n",
    "  if ((iter+1) % 200 == 0):\n",
    "    print('iter =  '+str(iter+1))\n",
    "    print('loss = {:.16f}'.format(loss_value))\n",
    "    print('loss_valid = {:.16f}'.format(loss(X_valid,Y_valid)))\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "## NN prediction on the test set\n",
    "fig,ax = plt.subplots(1,1,subplot_kw={'projection': '3d'})\n",
    "\n",
    "for k in range(100):\n",
    "    if k==0:\n",
    "        Y_plot = tf.expand_dims(Updatemodel(X_valid[:,:,0]),2)\n",
    "    else:\n",
    "        Y_plot = tf.concat( [Y_plot, tf.expand_dims(Updatemodel(Y_plot[:,:,-1]),2) ], 2 )\n",
    "\n",
    "j=1\n",
    "\n",
    "x = Y_valid.numpy()[j,0,:]\n",
    "y = Y_valid.numpy()[j,1,:]\n",
    "z = Y_valid.numpy()[j,2,:]\n",
    "ax.plot(x, y, z,linewidth=1)\n",
    "x = Y_plot.numpy()[j,0,:]\n",
    "y = Y_plot.numpy()[j,1,:]\n",
    "z = Y_plot.numpy()[j,2,:]\n",
    "ax.plot(x, y, z,'--',linewidth=1)\n",
    "    \n",
    "ax.view_init(18, -113)\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: implement a custum loss to perform a further training of the neural network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(X_train,Y_train):\n",
    "    \n",
    "    #Y_pred = Updatemodel(X_train)\n",
    "\n",
    "    # custom loss\n",
    "    # ....\n",
    "\n",
    "    # loss components\n",
    "    # mse_loss = ...\n",
    "\n",
    "    return mse_loss\n",
    "\n",
    "# neural network weight gradients\n",
    "@tf.function\n",
    "def grad(model,X_train,Y_train):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        loss_value = loss(X_train,Y_train)\n",
    "        grads = tape.gradient(loss_value,model.trainable_variables)\n",
    "    return loss_value, grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "tf_optimizer = tf.keras.optimizers.Adam(learning_rate=0.003,beta_1=0.99)\n",
    "\n",
    "for iter in range(10000):\n",
    "\n",
    "  # compute gradients using AD\n",
    "  loss_value,grads = grad(Updatemodel,X_train,Y_train)\n",
    "\n",
    "  # update neural network weights\n",
    "  tf_optimizer.apply_gradients(zip(grads,Updatemodel.trainable_variables))\n",
    "\n",
    "  # display intermediate results\n",
    "  if ((iter+1) % 200 == 0):\n",
    "    print('iter =  '+str(iter+1))\n",
    "    print('loss = {:.16f}'.format(loss_value))\n",
    "    print('loss_valid = {:.16f}'.format(loss(X_valid,Y_valid)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NN prediction on the test set\n",
    "fig,ax = plt.subplots(1,1,subplot_kw={'projection': '3d'})\n",
    "\n",
    "for k in range(100):\n",
    "    if k==0:\n",
    "        Y_plot = tf.expand_dims(Updatemodel(X_valid[:,:,0]),2)\n",
    "    else:\n",
    "        Y_plot = tf.concat( [Y_plot, tf.expand_dims(Updatemodel(Y_plot[:,:,-1]),2) ], 2 )\n",
    "\n",
    "j=0\n",
    "\n",
    "x = Y_valid.numpy()[j,0,:]\n",
    "y = Y_valid.numpy()[j,1,:]\n",
    "z = Y_valid.numpy()[j,2,:]\n",
    "ax.plot(x, y, z,linewidth=1)\n",
    "x = Y_plot.numpy()[j,0,:]\n",
    "y = Y_plot.numpy()[j,1,:]\n",
    "z = Y_plot.numpy()[j,2,:]\n",
    "ax.plot(x, y, z,'--',linewidth=1)\n",
    "    \n",
    "ax.view_init(18, -113)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Task: train using different configurations (and optmization settings)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "teachenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
